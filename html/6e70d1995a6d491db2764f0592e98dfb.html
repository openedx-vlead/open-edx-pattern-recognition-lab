<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">Linear perceptrons allow us to learn a decision boundary that would separate two classes. They are very effective when there are only two classes, and they are well separated. Such classifiers are referred to as discriminative classifiers.</p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">In contrast, generative classifiers consider each sample as a random vector, and explicity model each class by their distribution or density functions. To carry out the classification, we compute the likelihood that a given sample belong to each of the candidate classes, and assign the sample to the class that is most likely. In other words, we need to compute P(&omega;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">i</span>/x) for each class &omega;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">i</span>. However, the density functions provide only the likelihood of seeing a particular sample, given that the sample belongs to a specific class. i.e., the density functions provide us p(x/&omega;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">i</span>). The Bayes rule provides us with an approach to compute the likelihood of the class for a given sample, from the density functions and related information.</p>