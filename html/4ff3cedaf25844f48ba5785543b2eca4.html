<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">Assume that the samples of the two classes are linearly separable in the feature space. i.e., there exists a plane&nbsp;<span style="box-sizing: border-box; font-weight: bold;">w.x</span>+<i style="box-sizing: border-box;">w0 = 0</i>&nbsp;such that all samples belonging to the first class are on one side of the plane, and all samples of the second class are on the opposite side. If such planes exist, the goal of the perceptron algorithm is to learn any one such plane, given the data points. Once we learn the plane, it will be easy to classify new points in the future, as the points on one side of the plane will result in a positive value for&nbsp;<span style="box-sizing: border-box; font-weight: bold;">w.x</span>+<i style="box-sizing: border-box;">w0</i>, while points on the other side will give a negative value.</p>
<h3 style="box-sizing: border-box; font-family: Raleway; line-height: 1.1; color: #333333; margin-top: 20px; margin-bottom: 10px; font-size: 24px; text-transform: none;">Special Case: One dimensional feature vector</h3>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">To understand the working of the algorithm, we first take the simplest case, where the samples are represented by a single feature (one-dimensional).</p>