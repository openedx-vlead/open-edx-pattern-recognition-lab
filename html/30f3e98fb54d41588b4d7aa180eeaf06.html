<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">Consider the following quote from a 2000 article in the Economist on the Bayesian Approach [<a href="http://www.ai.mit.edu/~murphyk/Bayes/economist.html" style="box-sizing: border-box; background-color: transparent; color: #337ab7; text-decoration: none; outline: 0px;">link</a><span style="box-sizing: border-box; color: #000000;">]:</span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;"><i style="box-sizing: border-box;">"The essence of the Bayesian approach is to provide a mathematical rule explaining how you should change your existing beliefs in the light of new evidence. In other words, it allows us to combine new data with their existing knowledge or expertise. The canonical example is to imagine that a precocious newborn observes his first sunset, and wonders whether the sun will rise again or not. He assigns equal prior probabilities to both possible outcomes, and represents this by placing one white and one black marble into a bag. The following day, when the sun rises, the child places another white marble in the bag. The probability that a marble plucked randomly from the bag will be white (i.e., the child's degree of belief in future sunrises) has thus gone from a half to two-thirds. After sunrise the next day, the child adds another white marble, and the probability (and thus the degree of belief) goes from two-thirds to three-quarters. And so on. Gradually, the initial belief that the sun is just as likely as not to rise each morning is modified to become a near-certainty that the sun will always rise."</i></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;"><span style="box-sizing: border-box; color: #000000;">&nbsp;</span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">In terms of classification, the Bayes theorem allows us to combine prior probabilities, along with observed evidence to arrive at the posterior probability. More or less, conditional probabilities represent the probability of an event occurring given evidence. To better understand, Bayes Theorem can be derived from the joint probability of A and B (i.e.&nbsp;<i style="box-sizing: border-box;">P</i>(<i style="box-sizing: border-box;">A,B</i>)) as follows:</p>
<p><span style="box-sizing: border-box; color: #333333; font-family: Raleway; font-size: 10pt;"><span style="box-sizing: border-box; font-family: 'Times New Roman', serif;"></span></span><span style="color: #333333; font-family: Raleway; font-size: 14px;"></span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway; text-align: center;"><span style="box-sizing: border-box; color: #000000;"><img alt="ole.gif" border="0" height="101" src="http://cse20-iiith.vlabs.ac.in/exp5/images/ole.gif" width="217" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></span></p>
<p></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;"><span style="box-sizing: border-box; color: #000000;">where&nbsp;</span><span class="serifequation" style="box-sizing: border-box;"><span style="box-sizing: border-box; color: #000000;"><i style="box-sizing: border-box;"><span style="box-sizing: border-box; font-weight: bold;">P</span></i><span style="box-sizing: border-box; font-weight: bold;">(<i style="box-sizing: border-box;">A</i>|<i style="box-sizing: border-box;">B</i>)</span></span></span><span style="box-sizing: border-box; color: #000000;">&nbsp;is referred to as the&nbsp;<i style="box-sizing: border-box;">posterior</i>;&nbsp;<i style="box-sizing: border-box;"><span style="box-sizing: border-box; font-weight: bold;">P</span></i><span style="box-sizing: border-box; font-weight: bold;">(<i style="box-sizing: border-box;">B</i>|<i style="box-sizing: border-box;">A</i>)</span>&nbsp;is known as the&nbsp;<i style="box-sizing: border-box;">likelihood</i>,&nbsp;<i style="box-sizing: border-box;"><span style="box-sizing: border-box; font-weight: bold;">P</span></i><span style="box-sizing: border-box; font-weight: bold;">(<i style="box-sizing: border-box;">A</i>)</span>&nbsp;is the&nbsp;<i style="box-sizing: border-box;">prior</i>&nbsp;and&nbsp;<i style="box-sizing: border-box;"><span style="box-sizing: border-box; font-weight: bold;">P</span></i><span style="box-sizing: border-box; font-weight: bold;">(<i style="box-sizing: border-box;">B</i>)</span>&nbsp;is generally the&nbsp;<i style="box-sizing: border-box;">evidence</i>&nbsp;and is used as a scaling factor. Therefore, it is handy to remember Bayes Rule as:</span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway; text-align: center;"><span style="box-sizing: border-box; color: #000000;"><img alt="ole1.gif" border="0" height="47" src="http://cse20-iiith.vlabs.ac.in/exp5/images/ole1.gif" width="237" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;"><span style="box-sizing: border-box; color: #000000;">&nbsp;These terms will be discussed a little later.</span></p>
<p></p>