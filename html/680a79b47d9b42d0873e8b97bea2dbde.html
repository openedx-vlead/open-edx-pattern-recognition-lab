<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">As learned from the previous experiment, if we have a generative model for each of the classes, one can devlop a classifier that assigns an unknown sample to one of the possible set of classes.</p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">Suppose there is a sample x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">1</span>, x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">2</span>, ..., x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">n</span>&nbsp;of&nbsp;<i style="box-sizing: border-box;">n</i>&nbsp;iid observations, coming from a distribution with an unknown pdf &fnof;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>(&middot;). It is however surmised that the function &fnof;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;belongs to a certain family of distributions {&thinsp;&fnof;(&middot;|&theta;), &theta; &isin; &Theta;&thinsp;}, called the parametric model, so that &fnof;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;= &fnof;(&middot;|&theta;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>). The value &theta;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;is unknown and is referred to as the "true value" of the parameter. It is desirable to find some estimator&nbsp;<img src="http://cse20-iiith.vlabs.ac.in/exp6/images/thetahat.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" />&nbsp;which would be as close to the true value &theta;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;as possible. Both the observed variables x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;and the parameter &theta; can be vectors.</p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">To use the method of maximum likelihood, one first specifies the joint density function for all the the observations. For an Independent and Identically distributed distribution, this joint density function will be&nbsp;<br style="box-sizing: border-box;" /><img src="http://cse20-iiith.vlabs.ac.in/exp6/images/jointdistributionfunction.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">Now we look at this function from a different perspective by considering the observed values x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">1</span>, x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">2</span>, ..., x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">n</span>&nbsp;to be fixed &ldquo;parameters&rdquo; of this function, whereas &theta; will be the function&rsquo;s variable and allowed to vary freely. From this point of view this distribution function will be called the likelihood:&nbsp;<img src="http://cse20-iiith.vlabs.ac.in/exp6/images/likelihoodfunction.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">In practice it is often more convenient to work with the logarithm of the likelihood function, called the log-likelihood, or its scaled version, called the average log-likelihood:&nbsp;<img src="http://cse20-iiith.vlabs.ac.in/exp6/images/loglikelihood.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">The method of maximum likelihood estimates &theta;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>&nbsp;by finding a value of &theta; that maximizes&nbsp;<img src="http://cse20-iiith.vlabs.ac.in/exp6/images/lhat.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" />. This method of estimation is a maximum likelihood estimator (MLE) of &theta;<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">0</span>:&nbsp;<img src="http://cse20-iiith.vlabs.ac.in/exp6/images/mletheta.png" style="box-sizing: border-box; border: 0px; vertical-align: middle;" /></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">A MLE estimate is the same regardless of whether we maximize the likelihood or the log-likelihood function, since log is a monotone transformation. For many models, a maximum likelihood estimator can be found as an explicit function of the observed data x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">1</span>,..., x<span style="box-sizing: border-box; position: relative; font-size: 12px; line-height: 0; vertical-align: baseline; bottom: -0.25em;">n</span>. For many other models, however, no closed-form solution to the maximization problem is known or available, and a MLE has to be found numerically using optimization methods. For some problems, there may be multiple estimates that maximize the likelihood. For other problems, no maximum likelihood estimate exists (meaning that the log-likelihood function increases without attaining the supremum value).</p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">A maximum likelihood estimator coincides with the most probable Bayesian estimator given a uniform prior distribution on the parameters.</p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;"><span style="box-sizing: border-box; font-weight: bold;">Example for a Normal Distribution, Bernoulli distribution and Poisson distribution:<span style="box-sizing: border-box;"></span></span></p>
<p style="box-sizing: border-box; margin: 0px 0px 10px; font-size: 16px; color: #333333; font-family: Raleway;">See the page&nbsp;<a href="http://mathworld.wolfram.com/MaximumLikelihood.html" target="_blank" style="box-sizing: border-box; background-color: transparent; color: #337ab7; text-decoration: none; outline: 0px;">http://mathworld.wolfram.com/MaximumLikelihood.html</a></p>